{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bd06d7-4d1b-4d1d-a9d8-2c5bd883994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import necessary libraries for data processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# --- XGBOOST CHECK ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"WARNING: XGBoost is NOT installed. The ensemble will run with only Random Forest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2d5410-1e6b-4c77-8889-116aa4436acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING PROCESSED DATA FOR ENSEMBLE\n",
      "================================================================================\n",
      "Training samples: 44553 | Features: 25\n",
      "Test samples: 11139\n",
      "\n",
      "================================================================================\n",
      "TRAINING ENSEMBLE CLASSIFIER AND TESTING METHODS\n",
      "================================================================================\n",
      "\n",
      "--- Individual Model Validation Scores ---\n",
      "rf              - F1-score: 0.7487\n",
      "xgb             - F1-score: 0.7286\n",
      "\n",
      "Model weights (for soft voting): ['0.514', '0.486']\n",
      "\n",
      "================================================================================\n",
      "BEST PERFORMING ENSEMBLE: Optimized Blending (F1: 0.7763)\n",
      "================================================================================\n",
      "Retraining best ensemble on FULL training dataset...\n",
      "\n",
      "✓ Submission saved to data/submission_ensemble.csv\n",
      "   patient_id  has_copd_risk\n",
      "0       42427              0\n",
      "1       27412              0\n",
      "2       19283              1\n",
      "3       45261              1\n",
      "4       11155              1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data():\n",
    "    \"\"\"Load preprocessed feature matrices and targets from current directory/data/.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING PROCESSED DATA FOR ENSEMBLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load files assuming standard names from previous steps\n",
    "    try:\n",
    "        X_train = pd.read_csv('data/train_processed.csv')\n",
    "        y_train = pd.read_csv('data/train_processed.csv')[\"has_copd_risk\"]\n",
    "        X_test = pd.read_csv('data/test_processed.csv')\n",
    "        test_patient_ids = pd.read_csv('data/ids.csv')[\"patient_id\"]\n",
    "        \n",
    "        # Drop the target from the X_train features loaded above (if still present)\n",
    "        X_train = X_train.drop([\"has_copd_risk\"], axis=1, errors='ignore')\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"FATAL ERROR: Required processed files not found. Check FILE_PATH_PREFIX.\")\n",
    "        raise\n",
    "        \n",
    "    # --- Final Data Cleaning (Robustness Check) ---\n",
    "    for df in [X_train, X_test]:\n",
    "        if df.isnull().values.any():\n",
    "            df.fillna(0, inplace=True)\n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "    print(f\"Training samples: {X_train.shape[0]} | Features: {X_train.shape[1]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    return X_train, y_train, X_test, test_patient_ids\n",
    "\n",
    "\n",
    "def build_base_models(scale_pos_weight: float):\n",
    "    \"\"\"Build base models for the ensemble (RF + XGBoost).\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # Random Forest - tuned for diversity and performance\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        class_weight=\"balanced\", # Handles imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "    )\n",
    "    models.append((\"rf\", rf))\n",
    "    \n",
    "    # XGBoost - tuned for maximum performance with class imbalance\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=5, # Reduced depth for stability\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            scale_pos_weight=scale_pos_weight, # Critical for F1 optimization\n",
    "            random_state=42,\n",
    "            eval_metric=\"logloss\",\n",
    "            n_jobs=-1,\n",
    "            reg_alpha=0.1, \n",
    "            reg_lambda=2.0,\n",
    "        )\n",
    "        models.append((\"xgb\", xgb_model))\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "def build_ensemble(models: list, voting: str = \"soft\", weights: list = None):\n",
    "    \"\"\"Build voting ensemble from base models.\"\"\"\n",
    "    return VotingClassifier(\n",
    "        estimators=models,\n",
    "        voting=voting,\n",
    "        weights=weights,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stacking_ensemble(models: list):\n",
    "    \"\"\"Build stacking ensemble with Logistic Regression meta-learner.\"\"\"\n",
    "    meta_learner = LogisticRegression(\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    return StackingClassifier(\n",
    "        estimators=models,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5,\n",
    "        stack_method='predict_proba',\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_ensemble(X_train: pd.DataFrame, y_train: pd.Series, voting: str = \"soft\"):\n",
    "    \"\"\"Train the ensemble classifier and test all combination methods.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING ENSEMBLE CLASSIFIER AND TESTING METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. Calculate class weight for XGBoost\n",
    "    X_np = X_train.to_numpy(dtype=np.float32)\n",
    "    y_np = y_train.to_numpy(dtype=np.int64)\n",
    "    class_counts = np.bincount(y_np)\n",
    "    scale_pos_weight = class_counts[0] / class_counts[1] if class_counts[1] > 0 else 1.0\n",
    "\n",
    "    # 2. Split for internal validation\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_np,\n",
    "        y_np,\n",
    "        test_size=0.2,\n",
    "        stratify=y_np,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # 3. Build and Configure Base Models\n",
    "    base_models = build_base_models(scale_pos_weight)\n",
    "    \n",
    "    # 4. Individual Model Performance and Calculate Weights (for weighted voting)\n",
    "    model_scores = []\n",
    "    print(\"\\n--- Individual Model Validation Scores ---\")\n",
    "    for name, model in base_models:\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        f1 = f1_score(y_val, pred)\n",
    "        model_scores.append(f1)\n",
    "        print(f\"{name:15s} - F1-score: {f1:.4f}\")\n",
    "    \n",
    "    # Normalize scores to create weights\n",
    "    if sum(model_scores) > 0:\n",
    "        weights = [score / sum(model_scores) for score in model_scores]\n",
    "        # Squared normalization to boost influence of better models\n",
    "        weights = [w**2 for w in weights]\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "    else:\n",
    "        weights = [1.0 / len(base_models)] * len(base_models)\n",
    "    print(f\"\\nModel weights (for soft voting): {[f'{w:.3f}' for w in weights]}\")\n",
    "    \n",
    "    \n",
    "    # 5. Testing Different Ensemble Methods\n",
    "    ensemble_results = []\n",
    "    \n",
    "    # a. Weighted Voting\n",
    "    weighted_ensemble = build_ensemble(base_models, voting=voting, weights=weights)\n",
    "    weighted_ensemble.fit(X_tr, y_tr)\n",
    "    weighted_f1 = f1_score(y_val, weighted_ensemble.predict(X_val))\n",
    "    ensemble_results.append((\"Weighted Voting\", weighted_ensemble, weighted_f1))\n",
    "    \n",
    "    # b. Stacking\n",
    "    stacking_ensemble = build_stacking_ensemble(base_models)\n",
    "    stacking_ensemble.fit(X_tr, y_tr)\n",
    "    stacking_f1 = f1_score(y_val, stacking_ensemble.predict(X_val))\n",
    "    ensemble_results.append((\"Stacking Ensemble\", stacking_ensemble, stacking_f1))\n",
    "    \n",
    "    # c. Optimized Blending (Uses RF and XGBoost probabilities)\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        best_blend_f1, best_blend_weights, best_blend_threshold = _optimize_blending(base_models, X_tr, y_tr, X_val, y_val)\n",
    "        ensemble_results.append((\"Optimized Blending\", None, best_blend_f1, best_blend_weights, best_blend_threshold))\n",
    "    \n",
    "    \n",
    "    # 6. Select Best Ensemble Method\n",
    "    best_result = max(ensemble_results, key=lambda x: x[2])\n",
    "    best_name, best_ensemble, best_f1 = best_result[0], best_result[1], best_result[2]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"BEST PERFORMING ENSEMBLE: {best_name} (F1: {best_f1:.4f})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 7. Final Retraining on Full Dataset\n",
    "    if best_name == \"Optimized Blending\":\n",
    "        # Create custom blend class and train it\n",
    "        final_ensemble = _BlendedEnsemble(base_models, best_result[3], best_result[4])\n",
    "    else:\n",
    "        final_ensemble = best_ensemble\n",
    "        \n",
    "    print(\"Retraining best ensemble on FULL training dataset...\")\n",
    "    final_ensemble.fit(X_np, y_np)\n",
    "    \n",
    "    return final_ensemble\n",
    "\n",
    "\n",
    "def _optimize_blending(models, X_tr, y_tr, X_val, y_val):\n",
    "    \"\"\"Internal function to search for optimal blend weights and threshold.\"\"\"\n",
    "    rf_model = next(model for name, model in models if name == \"rf\")\n",
    "    xgb_model = next(model for name, model in models if name == \"xgb\")\n",
    "    \n",
    "    rf_model.fit(X_tr, y_tr)\n",
    "    xgb_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    rf_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "    xgb_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    best_blend_f1 = 0\n",
    "    best_blend_weights = (0.5, 0.5)\n",
    "    best_blend_threshold = 0.5\n",
    "    \n",
    "    # Search over weights and thresholds\n",
    "    for w1 in np.arange(0.2, 0.8, 0.05):\n",
    "        w2 = 1.0 - w1\n",
    "        blended_proba = w1 * rf_proba + w2 * xgb_proba\n",
    "        \n",
    "        for threshold in np.arange(0.4, 0.6, 0.02):\n",
    "            blended_pred = (blended_proba > threshold).astype(int)\n",
    "            blend_f1 = f1_score(y_val, blended_pred)\n",
    "            \n",
    "            if blend_f1 > best_blend_f1:\n",
    "                best_blend_f1 = blend_f1\n",
    "                best_blend_weights = (w1, w2)\n",
    "                best_blend_threshold = threshold\n",
    "                \n",
    "    return best_blend_f1, best_blend_weights, best_blend_threshold\n",
    "\n",
    "\n",
    "class _BlendedEnsemble:\n",
    "    \"\"\"Custom class to handle the final blending model.\"\"\"\n",
    "    def __init__(self, models, weights, threshold=0.5):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Train base models on full data\n",
    "        for _, model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        rf_proba = self.models[0][1].predict_proba(X)[:, 1]\n",
    "        xgb_proba = self.models[1][1].predict_proba(X)[:, 1]\n",
    "        \n",
    "        blended = self.weights[0] * rf_proba + self.weights[1] * xgb_proba\n",
    "        return (blended > self.threshold).astype(int)\n",
    "\n",
    "\n",
    "def make_predictions(ensemble: object, X_test: pd.DataFrame):\n",
    "    \"\"\"Return class predictions for the test set.\"\"\"\n",
    "    X_test_np = X_test.to_numpy(dtype=np.float32)\n",
    "    preds = ensemble.predict(X_test_np)\n",
    "    return preds, None\n",
    "\n",
    "\n",
    "def create_submission(\n",
    "    patient_ids: pd.Series,\n",
    "    predictions: np.ndarray,\n",
    "    output_path: str = \"submission/submission_ensemble.csv\",\n",
    "):\n",
    "    \"\"\"Create submission matching sample_submission format.\"\"\"\n",
    "    submission = pd.DataFrame(\n",
    "        {\n",
    "            \"patient_id\": patient_ids.values,\n",
    "            \"has_copd_risk\": predictions.astype(int),\n",
    "        }\n",
    "    )\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Submission saved to {output_path}\")\n",
    "    print(submission.head())\n",
    "    return submission\n",
    "\n",
    "\n",
    "def main():\n",
    "    X_train, y_train, X_test, patient_ids = load_data()\n",
    "    \n",
    "    # Train with soft voting (uses probabilities when available)\n",
    "    ensemble = train_ensemble(X_train, y_train, voting=\"soft\")\n",
    "    \n",
    "    predictions, _ = make_predictions(ensemble, X_test)\n",
    "    create_submission(patient_ids, predictions)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this script, ensure XGBoost is installed and your processed files are ready.\n",
    "    # Execute the main function:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f60ce-45bd-433e-9506-818902edf3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
